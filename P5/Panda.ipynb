{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# data display\n",
    "#\n",
    "def displayData(X):\n",
    "    num_plots = int(np.size(X, 0)**.5)\n",
    "    fig, ax = plt.subplots(num_plots, num_plots, sharex=True, sharey=True)\n",
    "    plt.subplots_adjust(left=0, wspace=0, hspace=0)\n",
    "    img_num = 0\n",
    "    for i in range(num_plots):\n",
    "        for j in range(num_plots):\n",
    "            # Convert column vector into 20x20 pixel matrix\n",
    "            # transpose\n",
    "            img = X[img_num, :].reshape(20, 20).T\n",
    "            ax[i][j].imshow(img, cmap='Greys')\n",
    "            ax[i][j].set_axis_off()\n",
    "            img_num += 1\n",
    "\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def displayImage(im):\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    image = im.reshape(20, 20).T\n",
    "    ax2.imshow(image, cmap='gray')\n",
    "    return (fig2, ax2)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    data = loadmat(file, squeeze_me=True)\n",
    "    x = data['X']\n",
    "    y = data['y']\n",
    "    return x,y\n",
    "\n",
    "def load_weights(file):\n",
    "    weights = loadmat(file)\n",
    "    theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "    return theta1, theta2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the one hot encoding... You must use OneHotEncoder function of the sklern library. \n",
    "Probably need to use reshape(-1, 1) to change size of the data\n",
    "\"\"\"\n",
    "def one_hot_encoding(Y):\n",
    "    oneHotEncoder = OneHotEncoder()\n",
    "    YEnc = oneHotEncoder.fit(Y.reshape(-1,1))\n",
    "    return YEnc.transform(Y.reshape(-1,1)).toarray()\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the accuracy metrics function\n",
    "\"\"\"\n",
    "def accuracy(P,Y):\n",
    "\treturn np.mean(P == Y)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# gradient checking\n",
    "#\n",
    "def debugInitializeWeights(fan_in, fan_out):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a layer with fan_in incoming connections and\n",
    "    fan_out outgoing connections using a fixed set of values.\n",
    "    \"\"\"\n",
    "\n",
    "    W = np.sin(np.arange(1, 1 + (1+fan_in)*fan_out))/10.0\n",
    "    W = W.reshape(fan_out, 1+fan_in, order='F')\n",
    "    return W\n",
    "\n",
    "\n",
    "def computeNumericalGradient(J, Theta1, Theta2):\n",
    "    \"\"\"\n",
    "    Computes the gradient of J around theta using finite differences and\n",
    "    yields a numerical estimate of the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    theta = np.append(Theta1, Theta2).reshape(-1)\n",
    "\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    perturb = np.zeros_like(theta)\n",
    "    tol = 1e-4\n",
    "\n",
    "    for p in range(len(theta)):\n",
    "        # Set perturbation vector\n",
    "        perturb[p] = tol\n",
    "        loss1 = J(theta - perturb)\n",
    "        loss2 = J(theta + perturb)\n",
    "\n",
    "        # Compute numerical gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2 * tol)\n",
    "        perturb[p] = 0\n",
    "\n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main program\n",
      "If your compute_gradient implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 3.75224e-11\n",
      "Test passed!\n",
      "If your compute_gradient implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 3.72493e-11\n",
      "Test passed!\n",
      "We assume that: random_state of train_test_split  = 0 alpha=1, num_iterations = 2000, test_size=0.33, seed=0 and epislom = 0.12 \n",
      "Test 1 Calculando para lambda = 0\n",
      "Iteration      1: Cost   6.9019   \n",
      "Iteration    201: Cost   0.7697   \n",
      "Iteration    401: Cost   0.5180   \n",
      "Iteration    601: Cost   0.4106   \n",
      "Iteration    801: Cost   0.3423   \n",
      "Iteration   1001: Cost   0.2931   \n",
      "Iteration   1201: Cost   0.2553   \n",
      "Iteration   1401: Cost   0.2252   \n",
      "Iteration   1601: Cost   0.2005   \n",
      "Iteration   1801: Cost   0.1799   \n",
      "Iteration   2000: Cost   0.1626   \n",
      "Calculate accuracy for lambda = 0.00000 : 0.92606 expected accuracy is aprox: 0.92606\n",
      "Test 2 Calculando para lambda = 0.5\n",
      "Iteration      1: Cost   6.9056   \n",
      "Iteration    201: Cost   0.8089   \n",
      "Iteration    401: Cost   0.5778   \n",
      "Iteration    601: Cost   0.4866   \n",
      "Iteration    801: Cost   0.4322   \n",
      "Iteration   1001: Cost   0.3955   \n",
      "Iteration   1201: Cost   0.3692   \n",
      "Iteration   1401: Cost   0.3494   \n",
      "Iteration   1601: Cost   0.3341   \n",
      "Iteration   1801: Cost   0.3220   \n",
      "Iteration   2000: Cost   0.3123   \n",
      "Calculate accuracy for lambda = 0.50000 : 0.92545 expected accuracy is aprox: 0.92545\n",
      "Test 3 Calculando para lambda = 1\n",
      "Iteration      1: Cost   6.9093   \n",
      "Iteration    201: Cost   0.8467   \n",
      "Iteration    401: Cost   0.6334   \n",
      "Iteration    601: Cost   0.5548   \n",
      "Iteration    801: Cost   0.5104   \n",
      "Iteration   1001: Cost   0.4818   \n",
      "Iteration   1201: Cost   0.4622   \n",
      "Iteration   1401: Cost   0.4481   \n",
      "Iteration   1601: Cost   0.4376   \n",
      "Iteration   1801: Cost   0.4294   \n",
      "Iteration   2000: Cost   0.4231   \n",
      "Calculate accuracy for lambda = 1.00000 : 0.92667 expected accuracy is aprox: 0.92667\n",
      "\n",
      "Comparación con sklearn MLPClassifier:\n",
      "\n",
      "Lambda = 0\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLambda = 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m     test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mComparación con sklearn MLPClassifier:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLambda = 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLambda = 0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, num_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mtest_with_sklearn\u001b[1;34m(X_train, y_train, X_test, y_test, alpha, lambda_, num_iter)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Convertir etiquetas one-hot a etiquetas simples\u001b[39;00m\n\u001b[0;32m     45\u001b[0m y_train_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m y_test_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Crear y configurar MLPClassifier\u001b[39;00m\n\u001b[0;32m     49\u001b[0m clf \u001b[38;5;241m=\u001b[39m MLPClassifier(\n\u001b[0;32m     50\u001b[0m     hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m,),    \u001b[38;5;66;03m# Número de neuronas en la capa oculta (igual que tu implementación)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m'\u001b[39m,       \u001b[38;5;66;03m# Función sigmoidal\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m               \u001b[38;5;66;03m# Semilla para reproducibilidad\u001b[39;00m\n\u001b[0;32m     57\u001b[0m )\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from MLP import MLP, target_gradient, costNN, MLP_backprop_predict\n",
    "from utils import load_data, load_weights,one_hot_encoding, accuracy\n",
    "from public_test import checkNNGradients,MLP_test_step\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test 1 to be executed in Main\n",
    "\"\"\"\n",
    "def gradientTest():\n",
    "    checkNNGradients(costNN,target_gradient,0)\n",
    "    checkNNGradients(costNN,target_gradient,1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test 2 to be executed in Main\n",
    "\"\"\"\n",
    "def MLP_test(X_train,y_train, X_test, y_test):\n",
    "    print(\"We assume that: random_state of train_test_split  = 0 alpha=1, num_iterations = 2000, test_size=0.33, seed=0 and epislom = 0.12 \")\n",
    "    print(\"Test 1 Calculando para lambda = 0\")\n",
    "    MLP_test_step(MLP_backprop_predict,1,X_train,y_train,X_test,y_test,0,2000,0.92606,2000/10)\n",
    "    print(\"Test 2 Calculando para lambda = 0.5\")\n",
    "    MLP_test_step(MLP_backprop_predict,1,X_train,y_train,X_test,y_test,0.5,2000,0.92545,2000/10)\n",
    "    print(\"Test 3 Calculando para lambda = 1\")\n",
    "    MLP_test_step(MLP_backprop_predict,1,X_train,y_train,X_test,y_test,1,2000,0.92667,2000/10)\n",
    "\n",
    "def test_with_sklearn(X_train, y_train, X_test, y_test, alpha, lambda_, num_iter):\n",
    "    \"\"\"\n",
    "    Compara el MLP de sklearn con tu implementación propia.\n",
    "    \n",
    "    Args:\n",
    "    X_train: Entradas de entrenamiento.\n",
    "    y_train: Salidas de entrenamiento (one-hot encoded).\n",
    "    X_test: Entradas de prueba.\n",
    "    y_test: Salidas de prueba (one-hot encoded).\n",
    "    alpha: Tasa de aprendizaje.\n",
    "    lambda_: Regularización (L2).\n",
    "    num_iter: Número de iteraciones.\n",
    "    \n",
    "    Returns:\n",
    "    Precisión obtenida con MLPClassifier.\n",
    "    \"\"\"\n",
    "      # Convertir etiquetas one-hot a etiquetas simples, si es necesario\n",
    "    if len(y_train.shape) > 1:  # y_train está en formato one-hot\n",
    "        y_train_labels = np.argmax(y_train, axis=1)\n",
    "    else:  # y_train ya está en formato de etiquetas simples\n",
    "        y_train_labels = y_train\n",
    "    \n",
    "    if len(y_test.shape) > 1:  # y_test está en formato one-hot\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "    else:  # y_test ya está en formato de etiquetas simples\n",
    "        y_test_labels = y_test\n",
    "    \n",
    "    # Crear y configurar MLPClassifier\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(25,),    # Número de neuronas en la capa oculta (igual que tu implementación)\n",
    "        activation='logistic',       # Función sigmoidal\n",
    "        solver='adam',               # Optimizador (ADAM por defecto en sklearn)\n",
    "        alpha=lambda_,               # Parámetro de regularización L2\n",
    "        learning_rate_init=alpha,    # Tasa de aprendizaje\n",
    "        max_iter=num_iter,           # Número máximo de iteraciones\n",
    "        random_state=0               # Semilla para reproducibilidad\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    clf.fit(X_train, y_train_labels)\n",
    "    \n",
    "    # Calcular precisión\n",
    "    accuracy = clf.score(X_test, y_test_labels)  # Evaluar en el conjunto de prueba\n",
    "    print(f\"Precisión con MLPClassifier de sklearn: {accuracy:.5f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Main program\")\n",
    "   \n",
    "    #Test 1\n",
    "    gradientTest()\n",
    "\n",
    "    x,y = load_data('data/ex3data1.mat')\n",
    "    y_one = one_hot_encoding(y)\n",
    "    X_train, X_test, y_train_one_hot,y_test_one_hot = train_test_split(x, y_one, test_size=0.33, random_state=0)\n",
    "    \n",
    "\n",
    "    y_test =np.argmax(y_test_one_hot,axis=1)\n",
    "    \n",
    "    \n",
    "    #Test 2\n",
    "    MLP_test(X_train, y_train_one_hot, X_test,y_test)\n",
    "\n",
    "    print(\"\\nComparación con sklearn MLPClassifier:\")\n",
    "    print(\"\\nLambda = 0\")\n",
    "    test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha=1, lambda_=0, num_iter=2000)\n",
    "    print(\"\\nLambda = 0.5\")\n",
    "    test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha=1, lambda_=0.5, num_iter=2000)\n",
    "    print(\"\\nLambda = 1\")\n",
    "    test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha=1, lambda_=1, num_iter=2000)\n",
    " \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
