{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# data display\n",
    "#\n",
    "def displayData(X):\n",
    "    num_plots = int(np.size(X, 0)**.5)\n",
    "    fig, ax = plt.subplots(num_plots, num_plots, sharex=True, sharey=True)\n",
    "    plt.subplots_adjust(left=0, wspace=0, hspace=0)\n",
    "    img_num = 0\n",
    "    for i in range(num_plots):\n",
    "        for j in range(num_plots):\n",
    "            # Convert column vector into 20x20 pixel matrix\n",
    "            # transpose\n",
    "            img = X[img_num, :].reshape(20, 20).T\n",
    "            ax[i][j].imshow(img, cmap='Greys')\n",
    "            ax[i][j].set_axis_off()\n",
    "            img_num += 1\n",
    "\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def displayImage(im):\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    image = im.reshape(20, 20).T\n",
    "    ax2.imshow(image, cmap='gray')\n",
    "    return (fig2, ax2)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    data = loadmat(file, squeeze_me=True)\n",
    "    x = data['X']\n",
    "    y = data['y']\n",
    "    return x,y\n",
    "\n",
    "def load_weights(file):\n",
    "    weights = loadmat(file)\n",
    "    theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "    return theta1, theta2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the one hot encoding... You must use OneHotEncoder function of the sklern library. \n",
    "Probably need to use reshape(-1, 1) to change size of the data\n",
    "\"\"\"\n",
    "def one_hot_encoding(Y):\n",
    "    oneHotEncoder = OneHotEncoder()\n",
    "    YEnc = oneHotEncoder.fit(Y.reshape(-1,1))\n",
    "    return YEnc.transform(Y.reshape(-1,1)).toarray()\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the accuracy metrics function\n",
    "\"\"\"\n",
    "def accuracy(P,Y):\n",
    "\treturn np.mean(P == Y)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# gradient checking\n",
    "#\n",
    "def debugInitializeWeights(fan_in, fan_out):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a layer with fan_in incoming connections and\n",
    "    fan_out outgoing connections using a fixed set of values.\n",
    "    \"\"\"\n",
    "\n",
    "    W = np.sin(np.arange(1, 1 + (1+fan_in)*fan_out))/10.0\n",
    "    W = W.reshape(fan_out, 1+fan_in, order='F')\n",
    "    return W\n",
    "\n",
    "\n",
    "def computeNumericalGradient(J, Theta1, Theta2):\n",
    "    \"\"\"\n",
    "    Computes the gradient of J around theta using finite differences and\n",
    "    yields a numerical estimate of the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    theta = np.append(Theta1, Theta2).reshape(-1)\n",
    "\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    perturb = np.zeros_like(theta)\n",
    "    tol = 1e-4\n",
    "\n",
    "    for p in range(len(theta)):\n",
    "        # Set perturbation vector\n",
    "        perturb[p] = tol\n",
    "        loss1 = J(theta - perturb)\n",
    "        loss2 = J(theta + perturb)\n",
    "\n",
    "        # Compute numerical gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2 * tol)\n",
    "        perturb[p] = 0\n",
    "\n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main program\n",
      "If your compute_gradient implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 3.75224e-11\n",
      "Test passed!\n",
      "If your compute_gradient implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 3.72493e-11\n",
      "Test passed!\n",
      "We assume that: random_state of train_test_split  = 0 alpha=1, num_iterations = 2000, test_size=0.33, seed=0 and epislom = 0.12 \n",
      "Test 1 Calculando para lambda = 0\n",
      "Iteration      1: Cost   6.9019   \n",
      "Iteration    201: Cost   0.7697   \n",
      "Iteration    401: Cost   0.5180   \n",
      "Iteration    601: Cost   0.4106   \n",
      "Iteration    801: Cost   0.3423   \n",
      "Iteration   1001: Cost   0.2931   \n",
      "Iteration   1201: Cost   0.2553   \n",
      "Iteration   1401: Cost   0.2252   \n",
      "Iteration   1601: Cost   0.2005   \n",
      "Iteration   1801: Cost   0.1799   \n",
      "Iteration   2000: Cost   0.1626   \n",
      "Calculate accuracy for lambda = 0.00000 : 0.92606 expected accuracy is aprox: 0.92606\n",
      "Test 2 Calculando para lambda = 0.5\n",
      "Iteration      1: Cost   6.9056   \n",
      "Iteration    201: Cost   0.8089   \n",
      "Iteration    401: Cost   0.5778   \n",
      "Iteration    601: Cost   0.4866   \n",
      "Iteration    801: Cost   0.4322   \n",
      "Iteration   1001: Cost   0.3955   \n",
      "Iteration   1201: Cost   0.3692   \n",
      "Iteration   1401: Cost   0.3494   \n",
      "Iteration   1601: Cost   0.3341   \n",
      "Iteration   1801: Cost   0.3220   \n",
      "Iteration   2000: Cost   0.3123   \n",
      "Calculate accuracy for lambda = 0.50000 : 0.92545 expected accuracy is aprox: 0.92545\n",
      "Test 3 Calculando para lambda = 1\n",
      "Iteration      1: Cost   6.9093   \n",
      "Iteration    201: Cost   0.8467   \n",
      "Iteration    401: Cost   0.6334   \n",
      "Iteration    601: Cost   0.5548   \n",
      "Iteration    801: Cost   0.5104   \n",
      "Iteration   1001: Cost   0.4818   \n",
      "Iteration   1201: Cost   0.4622   \n",
      "Iteration   1401: Cost   0.4481   \n",
      "Iteration   1601: Cost   0.4376   \n",
      "Iteration   1801: Cost   0.4294   \n",
      "Iteration   2000: Cost   0.4231   \n",
      "Calculate accuracy for lambda = 1.00000 : 0.92667 expected accuracy is aprox: 0.92667\n",
      "\n",
      "Comparación con sklearn MLPClassifier:\n",
      "\n",
      "Lambda = 0\n",
      "Precisión con MLPClassifier de sklearn: 0.92121\n",
      "\n",
      "Lambda = 0.5\n",
      "Precisión con MLPClassifier de sklearn: 0.91333\n",
      "\n",
      "Lambda = 1\n",
      "Precisión con MLPClassifier de sklearn: 0.89273\n"
     ]
    }
   ],
   "source": [
    "from MLP import MLP, target_gradient, costNN, MLP_backprop_predict\n",
    "from utils import load_data, load_weights,one_hot_encoding, accuracy\n",
    "from public_test import checkNNGradients,MLP_test_step\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test 1 to be executed in Main\n",
    "\"\"\"\n",
    "def gradientTest():\n",
    "    checkNNGradients(costNN,target_gradient,0)\n",
    "    checkNNGradients(costNN,target_gradient,1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test 2 to be executed in Main\n",
    "\"\"\"\n",
    "def MLP_test(X_train,y_train, X_test, y_test):\n",
    "    print(\"We assume that: random_state of train_test_split  = 0 alpha=1, num_iterations = 2000, test_size=0.33, seed=0 and epislom = 0.12 \")\n",
    "    print(\"Test 1 Calculando para lambda = 0\")\n",
    "    MLP_test_step(MLP_backprop_predict,1,X_train,y_train,X_test,y_test,0,2000,0.92606,2000/10)\n",
    "    print(\"Test 2 Calculando para lambda = 0.5\")\n",
    "    MLP_test_step(MLP_backprop_predict,1,X_train,y_train,X_test,y_test,0.5,2000,0.92545,2000/10)\n",
    "    print(\"Test 3 Calculando para lambda = 1\")\n",
    "    MLP_test_step(MLP_backprop_predict,1,X_train,y_train,X_test,y_test,1,2000,0.92667,2000/10)\n",
    "\n",
    "def test_with_sklearn(X_train, y_train, X_test, y_test, alpha, lambda_, num_iter):\n",
    "  \n",
    "    # Convertir etiquetas one-hot a etiquetas simples, si es necesario\n",
    "    if len(y_train.shape) > 1:  # y_train está en formato one-hot\n",
    "        y_train_labels = np.argmax(y_train, axis=1)\n",
    "    else:  # y_train ya está en formato de etiquetas simples\n",
    "        y_train_labels = y_train\n",
    "    \n",
    "    if len(y_test.shape) > 1:  # y_test está en formato one-hot\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "    else:  # y_test ya está en formato de etiquetas simples\n",
    "        y_test_labels = y_test\n",
    "    \n",
    "    # Crear y configurar MLPClassifier\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(25,),    # Número de neuronas en la capa oculta (igual que tu implementación)\n",
    "        activation='logistic',       # Función sigmoidal\n",
    "        solver='adam',               # Optimizador (ADAM por defecto en sklearn)\n",
    "        alpha=lambda_,               # Parámetro de regularización L2\n",
    "        learning_rate_init=alpha,    # Tasa de aprendizaje\n",
    "        max_iter=num_iter,           # Número máximo de iteraciones\n",
    "        epsilon = 0.12               # Epsilon    \n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    clf.fit(X_train, y_train_labels)\n",
    "    \n",
    "    # Calcular precisión\n",
    "    accuracy = clf.score(X_test, y_test_labels)  # Evaluar en el conjunto de prueba\n",
    "    print(f\"Precisión con MLPClassifier de sklearn: {accuracy:.5f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Main program\")\n",
    "   \n",
    "    #Test 1\n",
    "    gradientTest()\n",
    "\n",
    "    x,y = load_data('data/ex3data1.mat')\n",
    "    y_one = one_hot_encoding(y)\n",
    "    X_train, X_test, y_train_one_hot,y_test_one_hot = train_test_split(x, y_one, test_size=0.33, random_state=0)\n",
    "    \n",
    "\n",
    "    y_test =np.argmax(y_test_one_hot,axis=1)\n",
    "    \n",
    "    \n",
    "    #Test 2\n",
    "    MLP_test(X_train, y_train_one_hot, X_test,y_test)\n",
    "\n",
    "    print(\"\\nComparación con sklearn MLPClassifier:\")\n",
    "    print(\"\\nLambda = 0\")\n",
    "    test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha=1, lambda_=0, num_iter=2000)\n",
    "    print(\"\\nLambda = 0.5\")\n",
    "    test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha=1, lambda_=0.5, num_iter=2000)\n",
    "    print(\"\\nLambda = 1\")\n",
    "    test_with_sklearn(X_train, y_train_one_hot, X_test, y_test, alpha=1, lambda_=1, num_iter=2000)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
